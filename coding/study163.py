#!/usr/bin/python# -*- coding: utf-8 -*-__author__ = 'ziyi'import requestsfrom requests import RequestExceptionfrom bs4 import BeautifulSoupfrom selenium import webdriverimport timeimport pandas as pddef get_html(url):    soup = None    try:        response = requests.get(url, timeout=30)        if response.status_code == 200:            html = response.text            soup = BeautifulSoup(html, 'html.parser')        else:            print('获取不到页面内容', response.status_code,url)    except RequestException as e:        print(url, '该网站无法访问', e)    finally:        soup = soup    return soupdef get_ajax_html(url):    urls = []    driver = webdriver.Chrome(executable_path=r'/Users/lvzeqin/Applications/chromedriver')    driver.get(url)    driver.execute_script ('window.scrollTo(0, document.body.scrollHeight)')    time.sleep (2)    driver.execute_script ('window.scrollTo(document.body.scrollHeight,200)')    time.sleep (2)    a=driver.find_element_by_xpath('//*[@id="j-course-box"]/div/div[4]/div/div/div/ul/li[3]/a').text    # print(a)    # driver.find_element_by_xpath('//*[@id="auto-id-1527057202955"]/li[2]').click()    url_link = driver.find_elements_by_xpath('//div[@class="j-course-list"]//ul[@class="uc-course-list_ul"]/li')    for i, one_li in enumerate(url_link):        url = one_li.find_element_by_xpath('//div[@class="j-course-list"]//ul[@class="uc-course-list_ul"]/li[{}]//a['                                           '@class="j-href"]'.format(i+1)).get_attribute('href')        # print(url)        urls.append(url)    # driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')    # time.sleep (2)    # while driver.find_element_by_xpath('//li[@class="ux-pager_btn ux-pager_btn__next"]/a[@class="th-bk-main-gh"]'):    #     driver.find_element_by_xpath('//li[@class="ux-pager_btn ux-pager_btn__next"]/a[@class="th-bk-main-gh"]').click()    #     driver.execute_script('window.scrollTo(document.body.scrollHeight,200)')    #     time.sleep(2)    #     url_link = driver.find_elements_by_xpath('//div[@class="j-course-list"]//ul[@class="uc-course-list_ul"]/li')    #     for i, one_li in enumerate(url_link):    #         url = one_li.find_element_by_xpath(    #             '//div[@class="j-course-list"]//ul[@class="uc-course-list_ul"]/li[{}]//a['    #             '@class="j-href"]'.format(i + 1)).get_attribute('href')    #         print(url)    #         urls.append(url)    driver.close()    return urlsdef get_detail(url):    content = {}    df = pd.DataFrame({})    driver = webdriver.Chrome(executable_path=r'/Users/lvzeqin/Applications/chromedriver')    driver.get(url)    driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')    time.sleep(2)    driver.execute_script('window.scrollTo(document.body.scrollHeight,200)')    time.sleep(2)    try:        content['class_name'] = driver.find_element_by_xpath('//*[@class="u-coursetitle f-fl"]/h2/span').text    except Exception as e:        print(url, '没有对应课程的名称', e)    try:        content['study_people'] = driver.find_element_by_xpath('//*[@class="u-coursetitle f-fl"]/div/span[@class="hot '                                                           'f-fs0"]').text    except Exception as e:        print(url, '没有学习的成员人数', e)    try:        content['score'] = driver.find_element_by_xpath('//*[@class="u-coursetitle f-fl"]/div/span[3]').text    except Exception as e:        print(url, '没有课程的得分', e)    try:        content['teacher'] = driver.find_element_by_xpath('//div[@class="teacher"]').text    except Exception as e:        print(url, '没有讲师的信息', e)    try:        content['author'] = driver.find_element_by_xpath('//p[@class="lname f-thide j-lname"]').text    except Exception as e:        print(url, '没有讲师的具体内容', e)    try:        content['author_introduction'] = driver.find_element_by_xpath('//*[@class="ltxt j-ltxt f-richEditorText '                                                                      'edueditor_styleclass_0 edueditor_styleclass_1 '                                                                      'edueditor_styleclass_2"]').text    except Exception as e:        print(url, '没有讲师的具体介绍', e)    try:        content['price'] = driver.find_element_by_xpath('//span[@class="price"]').text    except Exception as e:        print(url, '没有课程的价格', e)    try:        content['intro_user'] = driver.find_element_by_xpath('//*[@id="j-course-briefintro"]/div[@class="m-cbi '                                                           'f-cb"]/div[@class="u-ctn-intro j-course-user-box"]').text    except Exception as e:        print(url, '没有课程面向客户的基本介绍', e)    try:        content['intro_class'] = driver.find_element_by_xpath('//*[@id="j-course-briefintro"]/div[@class="m-cbi '                                                           'f-cb"]/div[@class="u-ctn-intro u-ctn-intro-last"]').text    except Exception as e:        print(url, '没有课程的具体介绍信息', e)    try:        driver.find_element_by_xpath('//*[@class="j-courseheadTab"]/ul/li[2]/a').click()    except Exception as e:        print(url, '没有课程的目录信息', e)    time.sleep(2)    if driver.find_element_by_xpath('//*[@id="j-chapter-title"]'):        print('有目录的信息可以爬取')        # contents = driver.find_element_by_xpath('//*[@class="chapter"].div')        # for one_content in contents:        #     content['contents'] = one_content.find_element_by_xpath('')    print(content)    df = df.append(content, ignore_index=True)    driver.close()    return dfif __name__ == '__main__':    url = 'http://study.163.com/courses-search?keyword=python'    soup = get_html(url)    # print(soup)    # print(soup.select_one('class.ux-pager').text)    urls = get_ajax_html(url)    # print(urls)    for one in urls:        print(one)        get_detail(one)