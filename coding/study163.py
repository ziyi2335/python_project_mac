#!/usr/bin/python# -*- coding: utf-8 -*-__author__ = 'ziyi'import requestsfrom requests import RequestExceptionfrom bs4 import BeautifulSoupfrom selenium import webdriverimport timeimport pandas as pdfrom copy import deepcopydef get_html(url):    soup = None    try:        response = requests.get(url, timeout=30)        if response.status_code == 200:            html = response.text            soup = BeautifulSoup(html, 'html.parser')        else:            print('获取不到页面内容', response.status_code,url)    except RequestException as e:        print(url, '该网站无法访问', e)    finally:        soup = soup    return soupdef get_ajax_html(url):    urls = []    driver = webdriver.Chrome(executable_path=r'/Users/lvzeqin/Applications/chromedriver')    driver.get(url)    driver.execute_script ('window.scrollTo(0, document.body.scrollHeight)')    time.sleep (2)    driver.execute_script ('window.scrollTo(document.body.scrollHeight,200)')    time.sleep (2)    a=driver.find_element_by_xpath('//*[@id="j-course-box"]/div/div[4]/div/div/div/ul/li[3]/a').text    # print(a)    # driver.find_element_by_xpath('//*[@id="auto-id-1527057202955"]/li[2]').click()    url_link = driver.find_elements_by_xpath('//div[@class="j-course-list"]//ul[@class="uc-course-list_ul"]/li')    for i, one_li in enumerate(url_link):        url = one_li.find_element_by_xpath('//div[@class="j-course-list"]//ul[@class="uc-course-list_ul"]/li[{}]//a['                                           '@class="j-href"]'.format(i+1)).get_attribute('href')        # print(url)        urls.append(url)    # 获取下一页的url链接    driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')    time.sleep (2)    try:        check = driver.find_element_by_xpath('//li[@class="ux-pager_btn ux-pager_btn__next"]/a[@class="th-bk-main-gh"]')    except Exception:        check = None    while check:        try:            driver.find_element_by_xpath('//li[@class="ux-pager_btn ux-pager_btn__next"]/a[@class="th-bk-main-gh"]').click()            driver.execute_script('window.scrollTo(document.body.scrollHeight,200)')            time.sleep(2)            url_link = driver.find_elements_by_xpath('//div[@class="j-course-list"]//ul[@class="uc-course-list_ul"]/li')            for i, one_li in enumerate(url_link):                try:                    url = one_li.find_element_by_xpath(                    '//div[@class="j-course-list"]//ul[@class="uc-course-list_ul"]/li[{}]//a['                    '@class="j-href"]'.format(i + 1)).get_attribute('href')                except Exception:                    print('没有下一页的内容了')                print(i+1, url)                urls.append(url)        except Exception:            break    driver.close()    return urlsdef get_detail(url):    content = {}    df = pd.DataFrame({})    option = webdriver.ChromeOptions()    # option.add_argument('disable-infobars')    option.add_argument('headless')    driver = webdriver.Chrome(executable_path=r'/Users/lvzeqin/Applications/chromedriver', chrome_options=option)    driver.get(url)    time.sleep(10)    driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')    time.sleep(2)    driver.execute_script('window.scrollTo(document.body.scrollHeight,200)')    time.sleep(2)    content['class_url'] = url    try:        content['class_name'] = driver.find_element_by_xpath('//*[@class="u-coursetitle f-fl"]/h2/span').text.encode(            'gbk', 'ignore').decode('gbk')    except Exception as e:        print(url, '没有对应课程的名称', e)    try:        content['study_people'] = driver.find_element_by_xpath('//*[@class="u-coursetitle f-fl"]/div/span[@class="hot f-fs0"]').text.encode('gbk', 'ignore').decode('gbk')    except Exception as e:        print(url, '没有学习的成员人数', e)    try:        content['score'] = driver.find_element_by_xpath('//*[@class="u-coursetitle f-fl"]/div/span[3]').text.encode('gbk', 'ignore').decode('gbk')    except Exception as e:        print(url, '没有课程的得分', e)    try:        content['teacher'] = driver.find_element_by_xpath('//div[@class="teacher"]').text.encode(            'gbk', 'ignore').decode('gbk')    except Exception as e:        print(url, '没有讲师的信息', e)    try:        content['author'] = driver.find_element_by_xpath('//p[@class="lname f-thide j-lname"]').text.encode(            'gbk', 'ignore').decode('gbk')    except Exception as e:        print(url, '没有讲师的具体内容', e)    try:        content['author_introduction'] = driver.find_element_by_xpath('//*[@class="ltxt j-ltxt f-richEditorText '                                                                      'edueditor_styleclass_0 edueditor_styleclass_1 '                                                                      'edueditor_styleclass_2"]').text.encode('gbk', 'ignore').decode('gbk')    except Exception as e:        print(url, '没有讲师的具体介绍', e)    try:        content['price'] = driver.find_element_by_xpath('//span[@class="price"]').text.encode(            'gbk', 'ignore').decode('gbk')    except Exception as e:        print(url, '没有课程的价格', e)    try:        content['intro_user'] = driver.find_element_by_xpath('//*[@id="j-course-briefintro"]/div[@class="m-cbi '                                                           'f-cb"]/div[@class="u-ctn-intro j-course-user-box"]').text.encode(            'gbk', 'ignore').decode('gbk')    except Exception as e:        print(url, '没有课程面向客户的基本介绍', e)    try:        content['intro_class'] = driver.find_element_by_xpath('//*[@id="j-course-briefintro"]/div[@class="m-cbi '                                                           'f-cb"]/div[@class="u-ctn-intro u-ctn-intro-last"]').text.encode(            'gbk', 'ignore').decode('gbk')    except Exception as e:        print(url, '没有课程的具体介绍信息', e)    try:        driver.find_element_by_xpath('//*[@class="j-courseheadTab"]/ul/li[2]/a').click()    except Exception as e:        print(url, '没有课程的目录信息', e)    time.sleep(2)    if driver.find_element_by_xpath('//*[@id="j-chapter-title"]'):        print('有目录的信息可以爬取')        if driver.find_elements_by_xpath('//div[@class="m-chapterList f-pr"]/div[@class="chapter"]'):            chapters = driver.find_elements_by_xpath('//div[@class="m-chapterList f-pr"]/div[@class="chapter"]')            class_all = {}            for j, one_chapter in enumerate(chapters):                class_infor = []                class_infor_dict = {}                try:                    class_chapter = one_chapter.find_element_by_xpath('div[@class="chapterhead"]').text.encode(            'gbk', 'ignore').decode('gbk')                except Exception as e:                    class_chapter = content['class_name']                # print(j+1, class_chapter)                details = one_chapter.find_elements_by_xpath('div[@class="section"]')                for i, one_detail in enumerate(details):                    # print(j+1, i)                    class_infor_dict['class_number'] = one_detail.find_element_by_xpath(                        'span[@class="f-fl f-thide ks"]').text.encode(            'gbk', 'ignore').decode('gbk')                    class_infor_dict['class_name'] = one_detail.find_element_by_xpath('span[@class="f-fl f-thide '                                                                                      'ksname"]').text.encode(            'gbk', 'ignore').decode('gbk')                    try:                        class_infor_dict['class_time'] = one_detail.find_element_by_xpath('span[@class="f-fr ksinfo '                                                                                          'j-hoverhide"]/span[@class="f-fr f-thide kstime"]').text.encode('gbk', 'ignore').decode('gbk')                    except Exception as e:                        print(class_chapter, e)                    # print(class_infor_dict)                    class_infor.append(deepcopy(class_infor_dict))                class_all['{}'.format(class_chapter)] = class_infor            content['class_all'] = class_all    print(content)    df = df.append(content, ignore_index=True)    driver.close()    return dfif __name__ == '__main__':    url = 'http://study.163.com/courses-search?keyword=python'    soup = get_html(url)    # print(soup)    # print(soup.select_one('class.ux-pager').text)    urls = get_ajax_html(url)    # print(urls)    df_all = pd.DataFrame({})    for i, one in enumerate(urls):        print(one)        df_one = get_detail(one)        if i == 0:            df_one.to_csv('../data/study163/python_classes.csv', encoding='gbk', index=False, mode='w')        else:            df_one.to_csv('../data/study163/python_classes.csv', encoding='gbk', index=False, mode='a', header=None)